---
title: "Group 7 - Classification model for the default status"
author: "L. Becker, A. Chebatarova, A. Kandel, A.Kusche, R. Mizrak"
date: 'Sys.Date()'
output:
  pdf_document: default
  html_document:
    df_print: paged
---

## Initialization

```{r initialization, results="hide"}
source(file = "functions.R")
func_init_env()
```

```{r loading Rds file, results="hide"}
dataset <- func_data_load()
```

```{r preprocessing, echo=TRUE}
dataset <- func_data_prep(dataset)
```

## Preprocessing

```{r concluded loans results= FALSE}
# Make a data.table
setDT(dataset)

# filter out all observations with loan_status == Current
dataset <- dataset[loan_status != 'Current']

# Change all the loan status that are not "Fully Paid" to 1 = "Default"
dataset$defaulted[dataset$loan_status != "Fully Paid"] <- 1

# Chaning levels of defaulted to have "Default" and "Paying"
levels(dataset$defaulted) = c(1, 0)

# Change all the defaulted values that aren't "Default" to 
dataset$defaulted[is.na(dataset$defaulted)] <- 0

# Remove unused factor levels
dataset$defaulted <- factor(dataset$defaulted)


# # Removing Feature 
# dataset <- subset(dataset, select = -c(hardship_type, hardship_status, hardship_loan_status,settlement_status, verification_status_joint, hardship_flag, disbursement_method, debt_settlement_flag, application_type, purpose, pymnt_plan, verification_status, home_ownership, initial_list_status, term))
```

```{r}
dataset <- dummy.data.frame(dataset, names = c("application_type","debt_settlement_flag", "disbursement_method","hardship_flag","hardship_loan_status","hardship_type","home_ownership","initial_list_status","loan_status","purpose","term", "settlement_status"))

dataset <- dummy.data.frame(dataset, names = c("hardship_status","pymnt_plan", "settlement_status", "verification_status", "verification_status_joint"))
```

## Create Train and Testset

```{r validation set approach}
# setting a seed for reproducability
set.seed(7)

# random split into  train and test set, with a ratio of 20:80
trainIndex <- sample(1:nrow(dataset),0.8*nrow(dataset))

train.data <- dataset[trainIndex,]
test.data  <- dataset[-trainIndex,]

# creating a small subset of data for testing models
train_smallIndex  <- sample(1:nrow(train.data), 1000)
test_smallIndex   <- sample(1:nrow(test.data), 1000)

small_train <- dataset[train_smallIndex,]
small_test  <- dataset[test_smallIndex,]
```

# https://www.r-bloggers.com/partial-least-squares-in-r/
```{r}
# Compile cross-validation settings
set.seed(100)
myfolds <- createMultiFolds(train.data, k = 5, times = 10)
control <- trainControl("repeatedcv", index = myfolds, selectionFunction = "oneSE")
 
```

#Perform Partial Least Squares Regression
```{r}

# Train PLS model
mod1 <- train(defaulted ~ ., data = train.data,
 method = "pls",
 metric = "Accuracy",
 tuneLength = 20,
 trControl = control,
 preProc = c("zv","center","scale"))

```

#Perform Principal Component Analysis 
```{r}
# PCA-DA
mod2 <- train(defaulted ~ ., data = train.data,
method = "lda",
metric = "Accuracy",
trControl = control,
preProc = c("zv","center","scale","pca"))
 
plot(mod1)

```

# Compile models and compare performance
```{r, message=FALSE, warning=FALSE, results="hide"}

models <- resamples(list("PLS-DA" = mod1, "PCA-DA" = mod2))
bwplot(models, metric = "Accuracy")

plot(varImp(mod1), 10, main = "PLS-DA")
plot(varImp(mod2), 10, main = "PCA-DA")

```

```{r}
lm.fit <- lm(defaulted ~., data = dataset, type = )

lm.fit <- lm(defaulted ~., data = subset(dataset, select = -c(loan_status)))
summary(lm.fit)
```
Perform the classification using Decision tree.
```{r Fit a regression tree to predict the defaulted status}
tree.defaulted<- tree(defaulted ~., data = subset(dataset, select = -c(id_2)))
summary(tree.defaulted)
plot(tree.defaulted)
text(tree.defaulted, cex = 0.75)
# The text isn't showing up
```
Perform the classification using Logistic Regression.
```{r classification using Logistic Regression}
# Setting Seed
set.seed(7)
# Creating a Generalized Linear Model
fit_glm_defaulted <- glm(defaulted ~., family = binomial(link = "logit"), data= small_train, na = na.omit)

# Summary of theeneralized Linear Model
summary(fit_glm_defaulted)

# Anova
anova(fit_glm_defaulted, test="Chisq")
# The model could not converge, meaning that we should improve our variable selection.

# creating predictions for the Generalized Linear Model
glm.probs <- predict(fit_glm_defaulted, type = "response")
glm.probs[1:5]

# setting if the prob is higher than 0.5 = 1 (defaulted), if bellow = 0 (Paying)
glm.pred <- ifelse(glm.probs > 0.5, "1", "0")

# Calculating the mean of preds == response
mean(glm.pred == defaulted_int_rate)

# We get an acc rate of 0.6442497/ error rate of 35.6%

```
Perform the classification using KNN.
```{r k-NN, cache = TRUE}
# Set random seed.
set.seed(7)

# Creating test and train labels
train_labels <- small_train$defaulted
test_labels <- small_test$defaulted

# 
knn_train <- small_train
knn_test <- small_test

# droping defaulted columns
knn_train$defaulted <- NULL
knn_test$defaulted <- NULL

# Make predictions using knn: pred
pred <- knn(train = knn_train, test = knn_test, cl = train_labels, k = 5)

# Construct the confusion matrix: conf
conf <- table(test_labels, pred)

# Print out the confusion matrix
conf

# Define range and accs
range <- 1:round(0.2 * nrow(knn_train))
accs <- rep(0, length(range))
for (k in range) {

  # Make predictions using knn: pred
  pred <- knn(knn_train, knn_test, train_labels, k = k)

  # Construct the confusion matrix: conf
  conf <- table(test_labels, pred)

  # Calculate the accuracy and store it in accs[k]
  accs[k] <- sum(diag(conf)) / sum(conf)
}
# Plot the accuracies.
plot(range, accs, xlab = "k")

# Calculate the best k
which.max(accs)
# best k = 75

# ROC <- roc(pred, test_labels)
# plot(ROC, col = "red")
# auc(ROC)

# Estimate the test error rate
mean(pred != test_labels)

# Fit the k-NN model with k=75
set.seed(7)
pred <- knn(train = knn_train, test = knn_test, cl = train_labels, k = 75)
# Construct the confusion matrix: conf
conf <- table(test_labels, pred)
# Estimate the test error rate
mean(pred != test_labels)
# we get an error rate of 26.1%
```
Perform the classification using Random forest.
```{r }
# Set random seed.
set.seed(7)

defaulted_RF <-  foreach(ntree=rep(125, 4), .combine=combine, .packages='randomForest') %dopar% {

defaulted_RF <- randomForest(defaulted ~. , data=subset(dataset, select=c(-id_2)) , subset =  small_trainRows, importance=TRUE, do.trace = TRUE, ntree=ntree)
 }
defaulted_RF
summary(defaulted_RF)
plot(defaulted_RF)
fitted.int_rate=predict(defaulted_RF)

defaulted_RF_pred <- predict(defaulted_RF, newdata = dataset[small_testRows,])
view(defaulted_RF_pred)

defaulted_int_rate <- small_train$defaulted
ROC <- roc(defaulted_RF_pred, defaulted_int_rate)
plot(ROC, col = "red")
auc(ROC)
```
Compare the respective train and test error performances to select one of these approaches.
```{r }
# KNN
# We have an error rate of 26.1 % with Knn

# Logistic Regression
# We get an acc rate of 0.6442497/ error rate of 35.6% for 

```

Perform the prediction on the validation set and compute the confusion matrix.
```{r }

```

Conceptually compare your approach with a solution existing for this problem. (Default
prediction is a very well-known problem in literature).
```{r }

```
