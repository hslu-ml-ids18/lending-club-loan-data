---
title: "Group 7 - Classification model for the default status"
author: "L. Becker, A. Chebatarova, A. Kandel, A.Kusche, R. Mizrak"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

# Introduction @ Anastasia

What is the project about? Who are we, what we are going to do?


# Init @ Anastasia
sajdkfjsdlkf jlaksdjf klasdjflk skladfs
af sdjflkjsdakl fksad f
sdafklsdjklf sad
f asd
f dsfldsjfk sajdf
```{r Initilize the environment, warning=TRUE}

  # Clear objects from the workspace
  rm(list=(ls()))
  
  # load library to deal with packages
  library(pacman)

  # install and loading required packages
  pacman::p_load(party, dummies, ranger, data.table, rmarkdown, tidyverse, caret, pls, corrplot, randomForest, foreach, plyr, tidyverse, magrittr, dplyr, tibble, doMC, pROC, class,MLmetrics, tree)
```
   

# Data Extraction @ Alex

In order to avoid to have to work with the whole original dataset from https://www.kaggle.com/wendykan/lending-club-loan-data
dataset_7.Rds has been created as follows.

``` 
loan_df <- read.csv(file = "loan.csv", header = TRUE)
loan_df <- cbind(id_2 = rownames(loan_df), loan_df)
rownames(loan_df) <- 1:nrow(loan_df)
loan_df$id_2 <- as.numeric(loan_df$id_2)
dataset_7 <- loan_df[which(loan_df$id_2%%8+1== 7),]
saveRDS(dataset_7, file = "dataset_7.Rds")

dataset_7 <- readRDS(file = "dataset_7.Rds")
```


  dataset <- readRDS(file = "dataset_7.Rds")
  
  
# General Exploration



# Data Preprocessing

sort dataset by colunm names
  dataset = dataset[ , order(names(dataset))]
  
  #removing policy_code because always equals 1
  dataset = subset(dataset, select = -c(policy_code) )
  
  
  # Removing columns that have > 0.05 NAs
  dataset <- dataset[, -which(colMeans(is.na(dataset)) > 0.05)]
  
  # Omiting columns that only have NAs, ie. id and member_id.
  dataset <- dataset[,colSums(is.na(dataset))<nrow(dataset)]
  
  # we remove desc as it is descrbing the purpose of the loan given it's unstructured data we can't do much about it.
  # Given we have a status column and date for all the factors that 
  # emp_title has too many levels to be used 
  # zip_code has too many levels and is a covariance of addr_state, thus we can remove it.
  
  # Too keep: earliest_cr_line could be an interesting variable to dummy code, we could transform it to only hold the year and make it a numerical value. same is true for sec_app_earliest_cr_line

  # last_pymnt_d and next_pymnt_d will be removed
  # addr_state is remove as it's not significant
  # grade is remove as it's a covariance of sub_grade
  dataset <- subset(dataset, select = -c(id_2, desc, emp_title, issue_d, title, zip_code, last_pymnt_d, next_pymnt_d, last_credit_pull_d, hardship_end_date, hardship_start_date, payment_plan_start_date, debt_settlement_flag_date, settlement_date, addr_state, grade) )

  # sub_grade has more than 32 levels which is a hard limit for random forest.
  # We'll dummy code it to circument this
  levels_sub_grade <- levels(dataset$sub_grade)
  dataset$sub_grade<- as.numeric(mapvalues(dataset$sub_grade, levels_sub_grade, seq(from = 1, to = 35, by = 1)))
  levels(dataset$sub_grade)
  
  # sorting emp_lenght, and dummy coding.
  levels_emp_length <- levels(dataset$emp_length)
  dataset$emp_length <- ordered(dataset$emp_length, levels = c("n/a", "< 1 year", "1 year", "2 years", "3 years","4 years", "5 years","6 years", "7 years", "8 years", "9 years", "10+ years"))
  levels(dataset$emp_length)
  dataset$emp_length <- as.numeric(mapvalues(dataset$emp_length, levels_emp_length, c(-1, seq(from = 0, to = 10, by = 1))))
  levels(dataset$emp_length)
  
  # grouping earliest_cr_line by year
  dataset$earliest_cr_line <- as.integer(substring(dataset$earliest_cr_line, 5))
  
  # grouping earliest_cr_line by year
  dataset$sec_app_earliest_cr_line <- as.integer(substring(as.character(dataset$sec_app_earliest_cr_line), 5))
  dataset$sec_app_earliest_cr_line[is.na(dataset$sec_app_earliest_cr_line)] <- -1
  typeof(dataset$sec_app_earliest_cr_line)
  levels(dataset$sec_app_earliest_cr_line)
  
  levels(dataset$total_rec_late_fee)
  
  na_count <-sapply(dataset, function(y) sum(length(which(is.na(y)))))
  
  # changing the dataset as a tbl object.
  dataset <- as.tbl(dataset)
  # change NAs to 0 in integer columns
  dataset <- mutate_if(dataset, is.integer, ~replace(., is.na(.), -1))
  # change NAs to 0 in doubles columns
  
  dataset <- mutate_if(dataset, is.numeric, ~replace(., is.na(.), -1))
  # change NAs to 0 in strings columns
  
  dataset <- mutate_if(dataset, is.character, ~replace(., is.na(.), "NA"))
  
  # change NAs to 0 in strings columns
  
  # dataset <- mutate_if(dataset, is.factor, ~replace(., is.na(.), "NA"))
  
  # change columns to character¨
  # dataset$hardship_type <- as.character(dataset$hardship_type)
  # dataset$hardship_reason <- as.character(dataset$hardship_reason)
  # dataset$hardship_status <- as.character(dataset$hardship_status)
  # dataset$hardship_loan_status <- as.character(dataset$hardship_loan_status)
  # dataset$settlement_status <- as.character(dataset$settlement_status)
  # dataset$verification_status_joint <- as.character(dataset$verification_status_joint)
  # 
  # change empty character columns to "NA"
  # 
  dataset <- mutate_if(dataset, is.character, ~replace(., is.na(.), "NA"))
  # 
  
  # calculating the number of unique levels per column
  sapply(dataset, function(col) length(unique(col)))
  
  na_count <-sapply(dataset, function(y) sum(length(which(is.na(y)))))
  
  return(dataset)



-----------
   
# Part 1 - Regression Analysis

## Preparatory tasks:
### Create a copy of your dataset, eliminating the entries that have an “na” in the interest rate variable int_rate. (Interest rate is used as output variable).
### Apply the “validation set approach” to reserve a meaningful amount of data for the test phase.
### Using one of the approaches for model selection discussed in class, reduce the number of predictors. For interpretability reasons, start with approaches that conserve the original predictor space. If any useful significant subset is possible, use a base transformation.
### Compute the correlation matrix for the selected set of predictors and the output variable, if useful, also using graphical representation.
 
## Main task:
### Compare three different methods to perform regression, using the cross-validation method to compute the best parameters. Consider using some regularization for the parameters shrinkage. Test the train error rate, the CV error rate and the test error.
  
   @Axel
# Part 2 - Classification Analysis
## Preparatory tasks
### Our goal in the second part of the assignment is to predict if a new customer will be able to fully pay back their loans using a classification method. Thus, we concentrate on the "concluded lends" in the data set, i.e., on all lends whose loan_status is not Current. To this end, filter out all observations with loan_status == Current.
### For the remaining observations, check if the loan_status is “Fully Paid”. If not, change the value of loan_status to “DEFAULTED”
### Create a validation set. 


## Main tasks:

### Use Principal Component Analysis for base transformation and then compare it with the Partial Least Squares Regression result. Select the best base with cross validation, using the better of the two approaches.
### Perform the classification using KNN, Logistic Regression, Decision tree and Random forest.
### Compare the respective train and test error performances to select one of these approaches.
### Perform the prediction on the validation set and compute the confusion matrix.
### Conceptually compare your approach with a solution existing for this problem. (Default prediction is a very well-known problem in literature).

 @Alex
# Summary 







