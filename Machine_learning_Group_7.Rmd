---
title: "Group 7 - Classification model for the default status"
author: "L. Becker, A. Chebatarova, A. Kandel, A.Kusche, R. Mizrak"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

# Introduction @ Anastasia

What is the project about? Who are we, what we are going to do?


# Init @ Anastasia
sajdkfjsdlkf jlaksdjf klasdjflk skladfs
af sdjflkjsdakl fksad f
sdafklsdjklf sad
f asd
f dsfldsjfk sajdf
```{r Initilize the environment, warning=TRUE}

  # Clear objects from the workspace
  rm(list=(ls()))
  
  # load library to deal with packages
  library(pacman)

  # install and loading required packages
  pacman::p_load(party, dummies, ranger, data.table, rmarkdown, tidyverse, caret, pls, corrplot, randomForest, foreach, plyr, tidyverse, magrittr, dplyr, tibble, doMC, pROC, class,MLmetrics, tree)
```
   

# Data Extraction

In order to avoid to have to work with the whole original dataset from <https://www.kaggle.com/wendykan/lending-club-loan-data> dataset_7.Rds has been created as follows:

```{r extract subset, eval=FALSE}
# load full dataset containing all loan data from 2007-2015
loan_df <- read.csv(file = "loan.csv", header = TRUE)

# add column id_2 for unique id
loan_df <- cbind(id_2 = rownames(loan_df), loan_df)

# give each row a consecutively numbered id
rownames(loan_df) <- 1:nrow(loan_df)

# change column id_2 as numeric
loan_df$id_2 <- as.numeric(loan_df$id_2)

# subset full data by modulo operator based on our group_id = 7
dataset_7 <- loan_df[which(loan_df$id_2%%8+1== 7),]

# save subset dataset_7.Rds
saveRDS(dataset_7, file = "dataset_7.Rds")

```

Read the subset of data from the previous step 
```{r read prepared dataset_7.Rds, eval=FALSE}


dataset <- readRDS(file = "dataset_7.Rds")

```  

# General Exploration


# Data Preprocessing 

Preprocess the dataset, remove colums, check for na ... 

```{r}
  
  # sort dataset by colunm names, to facilite search
  dataset = dataset[ , order(names(dataset))]
  
  # Removing columns that have > 0.05 NAs
  dataset <- dataset[, -which(colMeans(is.na(dataset)) > 0.05)]
  
  # remove some columsn because of the reasons below 
  # to many levels: zip_code, emp_title
  # not_relevant: desc, id_2, addr_state, last_pymnt_d, next_pymnt_d, issue_d, title, last_credit_pull_d, hardship_end_date, hardship_start_date, payment_plan_start_date, debt_settlement_flag_date, settlement_date
  # same data in every colum: policy_code
  # covariance: grade (of sub_grade)

  dataset <- subset(dataset, select = -c(id_2, policy_code, desc, emp_title, issue_d, title, zip_code, last_pymnt_d, next_pymnt_d, last_credit_pull_d, hardship_end_date, hardship_start_date, payment_plan_start_date, debt_settlement_flag_date, settlement_date, addr_state, grade) )

  # sub_grade has more than 32 levels which is a hard limit for random forest.
  # We'll dummy code it to circument this
  levels_sub_grade <- levels(dataset$sub_grade)
  dataset$sub_grade<- as.numeric(mapvalues(dataset$sub_grade, levels_sub_grade, seq(from = 1, to = 35, by = 1)))
  levels(dataset$sub_grade)
  
  # sorting emp_lenght, and dummy coding.
  levels_emp_length <- levels(dataset$emp_length)
  dataset$emp_length <- ordered(dataset$emp_length, levels = c("n/a", "< 1 year", "1 year", "2 years", "3 years","4 years", "5 years","6 years", "7 years", "8 years", "9 years", "10+ years"))
  levels(dataset$emp_length)
  dataset$emp_length <- as.numeric(mapvalues(dataset$emp_length, levels_emp_length, c(-1, seq(from = 0, to = 10, by = 1))))
  levels(dataset$emp_length)
  
  # grouping earliest_cr_line by year
  dataset$earliest_cr_line <- as.integer(substring(dataset$earliest_cr_line, 5))
  
  # grouping earliest_cr_line by year
  dataset$sec_app_earliest_cr_line <- as.integer(substring(as.character(dataset$sec_app_earliest_cr_line), 5))
  dataset$sec_app_earliest_cr_line[is.na(dataset$sec_app_earliest_cr_line)] <- -1
  typeof(dataset$sec_app_earliest_cr_line)
  levels(dataset$sec_app_earliest_cr_line)
  
  levels(dataset$total_rec_late_fee)
  
  na_count <-sapply(dataset, function(y) sum(length(which(is.na(y)))))
  
  # changing the dataset as a tbl object.
  dataset <- as.tbl(dataset)
  # change NAs to 0 in integer columns
  dataset <- mutate_if(dataset, is.integer, ~replace(., is.na(.), -1))
  # change NAs to 0 in doubles columns
  
  dataset <- mutate_if(dataset, is.numeric, ~replace(., is.na(.), -1))
  # change NAs to 0 in strings columns
  
  dataset <- mutate_if(dataset, is.character, ~replace(., is.na(.), "NA"))
  
  # change NAs to 0 in strings columns
  
  # dataset <- mutate_if(dataset, is.factor, ~replace(., is.na(.), "NA"))
  
  # change columns to character¨
  # dataset$hardship_type <- as.character(dataset$hardship_type)
  # dataset$hardship_reason <- as.character(dataset$hardship_reason)
  # dataset$hardship_status <- as.character(dataset$hardship_status)
  # dataset$hardship_loan_status <- as.character(dataset$hardship_loan_status)
  # dataset$settlement_status <- as.character(dataset$settlement_status)
  # dataset$verification_status_joint <- as.character(dataset$verification_status_joint)
  # 
  # change empty character columns to "NA"
  # 
  dataset <- mutate_if(dataset, is.character, ~replace(., is.na(.), "NA"))
  # 
  
  # calculating the number of unique levels per column
  sapply(dataset, function(col) length(unique(col)))
  
  na_count <-sapply(dataset, function(y) sum(length(which(is.na(y)))))
  
  return(dataset)



```


-----------
   
# Part 1 - Regression Analysis

## Preparatory tasks:
### Create a copy of your dataset, eliminating the entries that have an “na” in the interest rate variable int_rate. (Interest rate is used as output variable).
## Initialization

```{r preprocessing, echo=TRUE}
# Creating a separate dataset_reg for regression
dataset_reg <- dataset

```
### Apply the “validation set approach” to reserve a meaningful amount of data for the test phase.
```{r validation set approach, cache=TRUE}
# setting a seed for reproducability
set.seed(7)
# We randomly split our dataframe in a train and test set
trainRows = sample(1:nrow(dataset_reg),0.5*nrow(dataset_reg))
testRows = nrow(dataset_reg) - trainRows
train = dataset_reg[trainRows,]
test = dataset_reg[testRows,]
# creating a small subset of data for quickly that models are runnining whithout any issues.
# This won't be used for the analysis
small_trainRows = sample(1:nrow(dataset_reg),0.1*nrow(dataset_reg))
small_testRows = sample(nrow(dataset_reg) - small_trainRows,0.1*nrow(dataset_reg))
small_train = dataset_reg[small_trainRows,]
small_test = dataset_reg[small_testRows,]
```
### Using one of the approaches for model selection discussed in class, reduce the number of predictors. For interpretability reasons, start with approaches that conserve the original predictor space. If any useful significant subset is possible, use a base transformation.

### Compute the correlation matrix for the selected set of predictors and the output variable, if useful, also using graphical representation.
```{r correlation matrix}
# correlation matrix by p-value
corrmatrix <- subset(dataset_reg, select = c(-id_2, -term, -home_ownership, -initial_list_status, -application_type, -verification_status_joint, -hardship_flag, -hardship_type, -hardship_reason, -hardship_type, -hardship_loan_status, -hardship_status, -disbursement_method, -debt_settlement_flag, -verification_status, -loan_status, -pymnt_plan, -purpose, -settlement_status))
mcor <- cor(corrmatrix)
corrplot(mcor, type="upper", sig.level = 0.001, insig = "blank",number.font = 0)
# correlogram combined with significance test <- only correlations that are relevant are shown in the plot
# still gives out na error<- how can we remove all nas to make it work?

## Data Exploration
```

```{r Data Exploration - interest rate vs loan amount}
# plotting the loan amount vs the interest rate
{ggplot(data = dataset_reg, mapping = aes(loan_amnt, int_rate)) +
  geom_point()}
# from this visualisation we can't see any clear correlation.
```

```{r Data Exploration - interest rate vs term}
# plotting the loan amount vs the term
{ggplot(data = dataset_reg, mapping = aes(term, int_rate)) +
  geom_boxplot()}
# Here we can see that the interest rate is generaly higher for 60 months loans and lower for 36 months loans.
```

```{r Data Exploration - interest rate vs grade}
# plotting the loan amount vs the grade
{ggplot(data = dataset_reg, mapping = aes(factor(sub_grade), int_rate), group = 2) +
  geom_boxplot()}
# The sub_grade seems to have a strong influence on the interest rate
```

```{r Data Exploration - interest rate vs home ownership}
# plotting the loan amount vs home ownership
{ggplot(data = dataset_reg, mapping = aes(factor(home_ownership), int_rate), group = 2) +
  geom_boxplot()}
# Here we can see that the interest rate is quite close to each other between home_ownership.
```

```{r Data Exploration - interest rate vs verification_status}
# plotting the loan amount vs verification_status
{ggplot(data = dataset_reg, mapping = aes(factor(verification_status), int_rate), group = 2) +
  geom_boxplot()}
# The verification_status seems to also have an influence on the interest rate.

```

```{r Data Exploration - interest rate vs emp_title}
# plotting the loan amount vs verification_status
{ggplot(data = dataset_reg, mapping = aes(factor(emp_title), int_rate), group = 2) +
  geom_boxplot()}
# The emp_title ????

```
```{r Data Exploration lm, cache=TRUE}
# lm on the whole dataset_reg
if (exists(x = "lmp.fit") == FALSE){
readRDS("int_rate_lmp_fit")}
   else
lmp.fit=lm(int_rate~.,data = subset(dataset_reg, select=c(-id_2)))

summary(lmp.fit)
plot(lmp.fit)
#removing features that do not have at least a 0.001 P value
dataset_reg <- subset(dataset_reg, select = -c(annual_inc, dti, earliest_cr_line,total_pymnt, total_rec_prncp, total_rec_int, total_rec_late_fee, recoveries, acc_open_past_24mths, avg_cur_bal, bc_open_to_buy, delinq_amnt, mo_sin_old_rev_tl_op, mo_sin_rcnt_rev_tl_op, mort_acc, num_il_tl, pct_tl_nvr_dlq, tax_liens, total_bal_ex_mort, total_bc_limit, total_il_high_credit_limit, num_actv_bc_tl, mo_sin_rcnt_tl))

# Donig a second round to see if further variables can be removed.
summary(lmp.fit)
lmp.fit=lm(int_rate~.,data = subset(dataset_reg, select=c(-id_2)))

#removing features that do not have at least a 0.001 P value

dataset_reg <- subset(dataset_reg, select = -c(hardship_reason, inq_last_6mths, num_actv_rev_tl, percent_bc_gt_75, tot_hi_cred_lim))
                  
lmp.fit=lm(int_rate~.,data = subset(dataset_reg, select=c(-id_2)))
summary(lmp.fit)

dataset_reg <- subset(dataset_reg, select = -c(bc_util))
                  
}}}

```

```{r}
lm.fit2=lm(int_rate~poly(sub_grade,2),data=dataset_reg)
lm.fit4=lm(int_rate~poly(sub_grade,4),data=dataset_reg)
lm.fit5=lm(int_rate~poly(sub_grade,5),data=dataset_reg)
lm.fit6=lm(int_rate~poly(sub_grade,6),data=dataset_reg)
lm.fit10=lm(int_rate~poly(sub_grade,10),data=dataset_reg)
anova(lm.fit,lm.fit2,lm.fit4,lm.fit5,lm.fit6,lm.fit10, na.action = na.omit)

```
## Main task:
### Compare three different methods to perform regression, using the cross-validation method to compute the best parameters. Consider using some regularization for the parameters shrinkage. Test the train error rate, the CV error rate and the test error.

```{r stepwise model, results = FALSE, cache=TRUE, include=FALSE}

# should we use a gaussian model here?
null_model <- glm(int_rate ~ 1, data = subset(dataset_reg[small_trainRows,], select=c(-id_2)), family = "gaussian")

# Specify the partial model using a subset of the potential predictors
partial_model <- glm(int_rate ~ ., data=subset(dataset_reg[small_trainRows,], select=c(-id_2)), family = "gaussian")

#  -emp_title, -hardship_type, -issue_d, -desc, -title, -zip_code, -earliest_cr_line, -total_rec_late_fee, -last_pymnt_d, -next_pymnt_d, -last_credit_pull_d, -policy_code, -sec_app_earliest_cr_line, -deferral_term, -hardship_length, -debt_settlement_flag_date, -settlement_date

# This is taking too much time to process, stoped this at the 12th perdictor.
# Use a forward stepwise algorithm to build a parsimonious model
step_model <- step(null_model, scope = list(lower = null_model, upper = partial_model), direction = "forward", trace = 1)
# #  int_rate ~ sub_grade + all_util + loan_status + total_pymnt +
#     installment + initial_list_status + total_pymnt_inv + verification_status +
#     loan_amnt + term + out_prncp_inv + inq_fi + last_pymnt_amnt +
#     num_tl_op_past_12m + open_rv_12m + open_il_12m + sec_app_revol_util +
#     num_bc_tl + out_prncp + max_bal_bc + pub_rec + total_bc_limit +
#     open_acc + num_sats + purpose + total_rec_prncp

# Estimate the stepwise probability
step_prob <- predict(step_model, type = "response")

# getting the step response
step_response <- dataset_reg[small_testRows,]$int_rate

# MAPE
MAPE(step_prob, step_response)
```

```{r stepwise model Both direction elimination , results = FALSE, cache=TRUE}
# should we use a gaussian model here?
null_model <- glm(int_rate ~ 1, data = subset(dataset_reg[small_trainRows,], select=c(-id_2)), family = "gaussian")

# Specify the partial model using a subset of the potential predictors
partial_model <- glm(int_rate ~ ., data=subset(dataset_reg[small_trainRows,], select=c(-id_2)), family = "gaussian")

step_model <- step(null_model, scope = list(lower = null_model, upper = partial_model), direction = "both", trace = 1)

saveRDS(step_model,"step_model_both")
# Estimate the stepwise probability for the test set.
step_prob <- predict(step_model, type = "response")
step_test_response <- dataset_reg[small_testRows,]$int_rate

step_prob <- predict(step_model, type = "response")
step_train_response <- dataset_reg[small_trainRows,]$int_rate
# MAPE for test set
MAPE(step_prob, step_test_response)
# MAPE for train set
MAPE(step_prob, step_train_response)

```

```{r, Ridge regression, results=FALSE}
library (car)
library (ridge)
library(lmridge)
vif(step_model)
model_testing <-lm(int_rate ~ sub_grade + out_prncp + open_acc + mo_sin_rcnt_rev_tl_op + total_rec_int +pct_tl_nvr_dlq + num_il_tl, data=small_train, na.action = na.omit)
predicted <- predict (model_testing, data=small_test)  # predict on test data
compare <- cbind (small_test$int_rate, predicted)  # combine actual and predicted
mean (apply(compare, 1, min)/apply(compare, 1, max))
linRidgeMod <- linearRidge(int_rate ~ sub_grade + out_prncp + open_acc + mo_sin_rcnt_rev_tl_op + total_rec_int +pct_tl_nvr_dlq + num_il_tl, data = small_train)
summary(linRidgeMod)
predicted <- predict(linRidgeMod, small_test)  # predict on test data
compare <- cbind (actual=small_test$int_rate, predicted)  # combine
mean (apply(compare, 1, min)/apply(compare, 1, max))
```

```{r Random Forest, results= False, cache=TRUE}
int_rate_RF <-  foreach(ntree=rep(125, 4), .combine=combine, .packages='randomForest')

int_rate_RF <- randomForest(int_rate ~., data=dataset_reg, subset = small_trainRows, importance=TRUE, do.trace = TRUE)

int_rate_RF
summary(int_rate_RF)

class.error
plot(int_rate_RF)
fitted.int_rate=predict(int_rate_RF)

int_rate_RF_pred <- predict(int_rate_RF, newdata = dataset_reg[small_testRows,])
view(int_rate_RF_pred)
table(int_rate_RF_pred, small_test$int_rate)
response_int_rate <- dataset_reg[small_trainRows,]$int_rate
ROC <- roc(int_rate_RF_pred, response_int_rate)
plot(ROC, col = "red")
auc(ROC)
MAPE()
saveRDS(int_rate_RF, file = "int_rate_RF")
```
```{r Random Forest Ranger, cache= TRUE}
model2 <- train(int_rate ~ sub_grade, data = dataset_reg[small_trainRows,], method = "ranger")
plot(model2)
```


```{r, Logistic regression, results=False}

fit_glm <- glm(int_rate ~. , data=subset(dataset_reg, select=c(-id_2), family = binomial))
summary(fit_glm)
glm.probs <- predict(fit_glm, type = "response")
glm.probs[1:5]
#No idea how to interpet it
```
  
   
# Part 2 - Classification Analysis

Our goal in the second part of the assignment is to predict if a new customer will be able to fully pay back their loans using a classification method. Thus, we concentrate on the "concluded lends" in the data set, i.e., on all lends whose loan_status is not Current. 

## Preparatory tasks

We filter out all observations with loan_status == Current.
For the remaining observations, we check if the loan_status is “Fully Paid”. If not, change the value of loan_status to “DEFAULTED”.

```{r}
# set dataset as data.table::datatable
setDT(dataset)

# filter out all observations with loan_status == Current
dataset <- dataset[loan_status != 'Current']

# change all the loan status that are not "Fully Paid" to 1
dataset$defaulted[dataset$loan_status != "Fully Paid"] <- 1

# change level of defaulted to 1 and 0 
levels(dataset$defaulted) = c(1, 0)

# Change all the defaulted values that aren't "Default" to 0
dataset$defaulted[is.na(dataset$defaulted)] <- 0

# remove origin variable, because defaulted is relevant now
dataset$loan_status <- NULL

# set defaulted as factor
dataset$defaulted <- as.factor(dataset$defaulted)

# confirm steps below, by checking results
table(dataset$defaulted)
```

### Create a validation set. 

```{r validation set approach}
# setting a seed for reproducability
set.seed(7)

# random split into  train and test set, with a ratio of 20:80
trainIndex <- sample(1:nrow(dataset),0.8*nrow(dataset))

train.data <- dataset[trainIndex,]
test.data  <- dataset[-trainIndex,]

# creating a small subset of data for testing models
train_smallIndex  <- sample(1:nrow(train.data), 1000)
test_smallIndex   <- sample(1:nrow(test.data), 1000)

small_train <- dataset[train_smallIndex,]
small_test  <- dataset[test_smallIndex,]

train.data <- small_train
test.data  <- small_test
```

## Main tasks:

Now we can go over to do the analysis on the dataset. Therefore we use differnt approaches for feature selection (PLS and PCA). Based on the results, we choose the features and do the classification analysis.   

### Use Principal Component Analysis for base transformation and then compare it with the Partial Least Squares Regression result. Select the best base with cross validation, using the better of the two approaches.

```{r}
# Compile cross-validation settings
set.seed(100)
myfolds <- createMultiFolds(train.data, k = 5, times = 10)
control <- trainControl("repeatedcv", index = myfolds, selectionFunction = "oneSE")
 
```

Perform Partial Least Squares Regression with caret package, to have a standarized handling.

```{r PLS analysis}

# Train PLS model
mod1 <- train(defaulted ~ ., data = train.data,
 method = "pls",
 metric = "Accuracy",
 tuneLength = 20,
 trControl = control,
 preProc = c("zv","center","scale"))

plot(mod1)

```

Perform Principal Component Analysis with caret package, to have a standarized handling.

```{r PCA analysis}
# PCA-DA
mod2 <- train(defaulted ~ ., data = train.data,
method = "lda",
metric = "Accuracy",
trControl = control,
preProc = c("zv","center","scale","pca"))
```

Compile models and compare performance

```{r, message=FALSE, warning=FALSE, results="hide"}

models <- resamples(list("PLS-DA" = mod1, "PCA-DA" = mod2))
bwplot(models, metric = "Accuracy")

plot(varImp(mod1), mod1$bestTune$ncomp, main = "PLS-DA")
#plot(varImp(mod2), 5, main = "PCA-DA")

```

# based on the results from PLS analysis in the step before, limit the dataset to the most relevant variables. 

```{r eval=FALSE, results="hide"}

total_rec_prncp, recoveries, collection_recovery_fee, last_pymnt_amnt, total_pymnt_inv, total_pymnt, debt_settlement_flag, sub_grade, int_rate, out_prncp, out_prncp_inv, settlement_status, term

train.data <- subset(train.data, select = c(defaulted, total_rec_prncp, recoveries, collection_recovery_fee, last_pymnt_amnt, total_pymnt_inv, total_pymnt, debt_settlement_flag, sub_grade, int_rate, out_prncp, out_prncp_inv, settlement_status, term))
test.data  <- subset(test.data, select = c(defaulted, total_rec_prncp, recoveries, collection_recovery_fee, last_pymnt_amnt, total_pymnt_inv, total_pymnt, debt_settlement_flag, sub_grade, int_rate, out_prncp, out_prncp_inv, settlement_status, term))
```

### Perform the classification using KNN, Logistic Regression, Decision tree and Random forest.

Train a model with KNN

```{r Perform KNN with caret, include=FALSE}
## knn
fit.knn  <- train(defaulted ~ ., 
                data = train.data, 
                method = "knn",
                trControl = control,
                preProc = c("zv","center","scale")
                ) 

```

Train a model with Logistic Regression

```{r Perform Logistic Regression with caret, include=FALSE}
## logistic regression
fit.lreg <-train(defaulted ~ recoveries + collection_recovery_fee + total_rec_prncp + last_pymnt_amnt + total_pymnt, 
                data = train.data, 
                method = "glm",
                trControl = control,
                family=binomial(),
                preProc = c("zv","center","scale")
                ) 

```

Train a model with Decision Trees

```{r Perform Decision tree with caret, include=FALSE}
## decision tree 
fit.dtree<-train(defaulted ~ recoveries + collection_recovery_fee + total_rec_prncp + last_pymnt_amnt + total_pymnt, 
               data=train.data,
               method="ctree",
               trControl = control,
               preProc = c("zv","center","scale"))
```

Train a model with Random Forest

```{r Perform Random Forest with caret, include=FALSE}
## random forest 
fit.rf<-train(defaulted ~ recoveries + collection_recovery_fee + total_rec_prncp + last_pymnt_amnt + total_pymnt, 
               data=train.data,
               method="rf",
               trControl = control,
               preProc = c("zv","center","scale"))
```

### Compare the respective train and test error performances to select one of these approaches.

```{r ROC Plot}
par(pty = "s")
roc(test.data$defaulted, as.numeric(predict(fit.dtree, test.data)), plot=TRUE, legacy.axes=TRUE, percent=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage", col="#377eb8", lwd=4, print.auc = TRUE, print.auc.y=80, print.auc.x=30)

plot.roc(test.data$defaulted, as.numeric(predict(fit.rf, test.data)), percent=TRUE, col="#b8377e", lwd=4, print.auc=TRUE, add=TRUE, print.auc.y=70, print.auc.x=30)

plot.roc(test.data$defaulted, as.numeric(predict(fit.dtree, test.data)), percent=TRUE, col="#b87137", lwd=4, print.auc=TRUE, add=TRUE, print.auc.y=60, print.auc.x=30)

plot.roc(test.data$defaulted, as.numeric(predict(fit.knn, test.data)), percent=TRUE, col="#7eb837", lwd=4, print.auc=TRUE, add=TRUE, print.auc.y=50, print.auc.x=30)
	
	legend("bottomright", legend=c("Decission Tree", "Random Forest", "Logisitic Regression", "KNN"), col=c("#377eb8", "#b8377e", "#b87137", "#7eb837"), lwd=4)
	
```

### Perform the prediction on the validation set and compute the confusion matrix.

```{r Perform Confusion Matrix for each method}

#Confusion Matrix KNN
confusionMatrix( predict(fit.knn, test.data), test.data$defaulted)    #Test

#Confusion Matrix Logistic Regression
confusionMatrix( predict(fit.lreg, test.data), test.data$defaulted)   #Test

#Confusion Matrix Decision Trees
confusionMatrix( predict(fit.dtree, test.data), test.data$defaulted)  #Test

#Confusion Matrix Random Forest
confusionMatrix( predict(fit.rf, test.data), test.data$defaulted)     #Test
```


### Conceptually compare your approach with a solution existing for this problem. (Default prediction is a very well-known problem in literature).

@ToDo
 
# Summary 









@ToDo