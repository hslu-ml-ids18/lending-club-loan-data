---
title: "Group 7 - Groupwork on leding club loan data"
author: "L. Becker, A. Chebatarova, A. Kandel, A.Kusche, R. Mizrak"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

# Introduction

Our data comes from Lending Club (LC) which is a peer-to-peer online lending platform. The data contains information on the loans issued by LC including the details of the loans such as loan description, interest rate applied, if the debt was fully paid, date of last payment etc. Our goal is to find appropriate models for predicting interest rate for each request, as well as perform classification model for setting reliable default status.

URL to source data: https://kaggle.com/wendykan/lending-club-loan-data

Required steps:

- Initialization of the environment. 
- Data exploration through data visualization which helps to understand our dataset and its characteristics.
- Data preprocessing which includes omitting unnecessary columns, sorting and grouping, reformatting and other actions required for making our data adequate for performing further analysis and modeling.
- Part 1: Regression Analysis - we get into details with our data to define meaningful amount of data, rational predictors, check correlations and determine models for prediction of interest rate and validate them.
 
- Part 2 - Classification Analysis - we take a step back in order to be sure that all necessary variables are included into our analysis, perform required transformations, define the models, check the errors and validate the results. 
- Summary 

# Initializing the environment

At this step we clear the workspace and install necessary packages for data processing.
```{r Initilize the environment, warning=TRUE}

  # Clear objects from the workspace
  rm(list=(ls()))
  
  # load library to deal with packages
  library(pacman)

  # install and loading required packages
  pacman::p_load(import, monomvn, party, dummies, ranger, data.table, rmarkdown, tidyverse, 
  caret, pls, corrplot, randomForest, foreach, plyr, tidyverse, magrittr, dplyr, tibble, doMC, 
  pROC, class,MLmetrics, tree, car, ridge, lmridge, xgboost)
```

\pagebreak   

# Data Extraction

In order to avoid to have to work with the whole original dataset from kaggle dataset_7.Rds has been created as follows:

```{r extract subset, eval=FALSE, include=FALSE}
# load full dataset containing all loan data from 2007-2015
loan_df <- read.csv(file = "loan.csv", header = TRUE)

# add column id_2 for unique id
loan_df <- cbind(id_2 = rownames(loan_df), loan_df)

# give each row a consecutively numbered id
rownames(loan_df) <- 1:nrow(loan_df)

# change column id_2 as numeric
loan_df$id_2 <- as.numeric(loan_df$id_2)

# subset full data by modulo operator based on our group_id = 7
dataset_7 <- loan_df[which(loan_df$id_2%%8+1== 7),]

# save subset dataset_7.Rds
saveRDS(dataset_7, file = "dataset_7.Rds")

```

Read the subset of data from the previous step 
```{r read prepared dataset_7.Rds}

# Read the RDS file
dataset <- readRDS(file = "dataset_7.Rds")
# Setting seed
set.seed(3452)
dataset <- dataset[sample(1:nrow(dataset),20000),]

```  

# Data Preprocessing 

Preprocess the dataset, remove columns, check for na ... 

```{r message=FALSE, warning=FALSE}
  
  # sort dataset by colunm names, to facilite search
  dataset = dataset[ , order(names(dataset))]
  
  # Removing columns that have > 0.05 NAs
  dataset <- dataset[, -which(colMeans(is.na(dataset)) > 0.05)]
  
  # remove some columns because of the reasons below 
  # to many levels: zip_code, emp_title
  # not_relevant: desc, id_2, addr_state, last_pymnt_d, next_pymnt_d, issue_d, title, last_credit_pull_d, hardship_end_date, hardship_start_date, payment_plan_start_date, debt_settlement_flag_date, settlement_date
  # same data in every colum: policy_code
  # covariance: grade (of sub_grade)
  
  dataset <- subset(dataset, select = -c(id_2, policy_code, desc, emp_title, issue_d, title, zip_code, last_pymnt_d, next_pymnt_d, last_credit_pull_d, hardship_end_date, hardship_start_date, payment_plan_start_date, debt_settlement_flag_date, settlement_date, addr_state, grade) )

  # sub_grade has more than 32 levels which is a hard limit for random forest.
  # We'll dummy code it to circument this
  levels_sub_grade <- levels(dataset$sub_grade)
  dataset$sub_grade<- as.numeric(mapvalues(dataset$sub_grade, levels_sub_grade, seq(from = 1, to = 35, by = 1)))

  
  # sorting emp_lenght, and dummy coding.
  levels_emp_length <- levels(dataset$emp_length)
  dataset$emp_length <- ordered(dataset$emp_length, levels = c("n/a", "< 1 year", "1 year", "2 years", "3 years","4 years", "5 years","6 years", "7 years", "8 years", "9 years", "10+ years"))
  levels(dataset$emp_length)
  dataset$emp_length <- as.numeric(mapvalues(dataset$emp_length, levels_emp_length, c(-1, seq(from = 0, to = 10, by = 1))))

  
  # grouping earliest_cr_line by year
  dataset$earliest_cr_line <- as.integer(substring(dataset$earliest_cr_line, 5))
  
  # grouping earliest_cr_line by year
  dataset$sec_app_earliest_cr_line <- as.integer(substring(as.character(dataset$sec_app_earliest_cr_line), 5))
  dataset$sec_app_earliest_cr_line[is.na(dataset$sec_app_earliest_cr_line)] <- -1
  typeof(dataset$sec_app_earliest_cr_line)
  
  na_count <-sapply(dataset, function(y) sum(length(which(is.na(y)))))
  
  # changing the dataset as a tbl object.
  dataset <- as.tbl(dataset)
  
  # change NAs to -1 in integer columns
  dataset <- mutate_if(dataset, is.integer, ~replace(., is.na(.), -1))
  
  # change NAs to -1 in numeric columns
    dataset <- mutate_if(dataset, is.numeric, ~replace(., is.na(.), -1))
  
  # change Missing values to "NA" in strings columns
  dataset <- mutate_if(dataset, is.character, ~replace(., is.na(.), "NA"))
  
  # change Missing Values to "NA" in strings columns
    dataset <- mutate_if(dataset, is.factor, ~replace(., is.na(.), "NA"))
  
 
  # calculating the number of unique levels per column
  sapply(dataset, function(col) length(unique(col)))
  
  na_count <-sapply(dataset, function(y) sum(length(which(is.na(y)))))
  
  #check data after processing
  head(dataset)
  dim(dataset)
  str(dataset)
  
```


\pagebreak 
   
# Part 1 - Regression Analysis

## Preparatory tasks:
### Create a copy of your dataset, eliminating the entries that have an “na” in the interest rate variable int_rate. (Interest rate is used as output variable).
## Initialization

```{r preprocessing, echo=TRUE}
# Creating a separate dataset_reg for regression
dataset_reg <- dataset

```



### Using one of the approaches for model selection discussed in class, reduce the number of predictors. For interpretability reasons, start with approaches that conserve the original predictor space. If any useful significant subset is possible, use a base transformation.

### Compute the correlation matrix for the selected set of predictors and the output variable, if useful, also using graphical representation.
```{r correlation matrix, include=FALSE}
# correlation matrix by p-value, removing non-numericals for this. 
corrmatrix <- subset(dataset_reg, select = c(-term, -home_ownership, -initial_list_status, -application_type, -verification_status_joint, -hardship_flag, -hardship_type, -hardship_loan_status, -hardship_status, -disbursement_method, -debt_settlement_flag, -verification_status, -loan_status, -pymnt_plan, -purpose, -settlement_status, -acc_now_delinq, -hardship_reason))
mcor <- cor(corrmatrix)
view(mcor)
# From the correlation matrix we can observe that for int_rate, sub_grade is the most significant factor with (0.976), followed by revol_util with (0.265).

# We can plot the correlation matrix to visually inspect it.

corrplot(mcor, type="upper", sig.level = 0.001, insig = "blank",number.font = 0)

# Here we can observe that there is some covariance between our factors, indicated by darker blue or red points.
# correlogram combined with significance test <- only correlations that are relevant are shown in the plot.
```
## Data Exploration

```{r Data Exploration - interest rate vs loan amount}
# plotting the loan amount vs the interest rate
{ggplot(data = dataset_reg, mapping = aes(loan_amnt, int_rate)) +
  geom_point()}
# from this visualisation we can't see any clear correlation.
```

```{r Data Exploration - interest rate vs term}
# plotting the loan amount vs the term
{ggplot(data = dataset_reg, mapping = aes(term, int_rate)) +
  geom_boxplot()}
# Here we can see that the interest rate is generaly higher for 60 months loans and lower for 36 months loans.
```

```{r Data Exploration - interest rate vs grade}
# plotting the loan amount vs the grade
{ggplot(data = dataset_reg, mapping = aes(factor(sub_grade), int_rate), group = 2) +
  geom_boxplot()}
# The sub_grade seems to have a strong correlation with the interest rate
```

```{r Data Exploration - interest rate vs home ownership}
# plotting the loan amount vs home ownership
{ggplot(data = dataset_reg, mapping = aes(factor(home_ownership), int_rate), group = 2) +
  geom_boxplot()}
# Here we can see that the interest rate is quite close to each other between home_ownership.
```

```{r Data Exploration - interest rate vs verification_status}
# plotting the loan amount vs verification_status
{ggplot(data = dataset_reg, mapping = aes(factor(verification_status), int_rate), group = 2) +
  geom_boxplot()}
# The verification_status seems to also have correlation with the interest rate.

```


```{r Data Exploration lm, cache=TRUE, eval=FALSE}
# lm on the whole dataset_reg
lmp.fit=lm(int_rate~.,data = dataset_reg)

summary(lmp.fit)
plot(lmp.fit)

#removing features that do not have at least a 0.001 P value
dataset_reg <- subset(dataset_reg, select = -c(annual_inc, dti, earliest_cr_line,total_pymnt, total_rec_prncp, total_rec_int, total_rec_late_fee, recoveries, acc_open_past_24mths, avg_cur_bal, bc_open_to_buy, delinq_amnt, mo_sin_old_rev_tl_op, mo_sin_rcnt_rev_tl_op, mort_acc, num_il_tl, pct_tl_nvr_dlq, tax_liens, total_bal_ex_mort, total_bc_limit, total_il_high_credit_limit, num_actv_bc_tl, mo_sin_rcnt_tl))

# Doing a second round to see if further variables can be removed.
summary(lmp.fit)
lmp.fit=lm(int_rate~.,data = dataset_reg)

#removing features that do not have at least a 0.001 P value

dataset_reg <- subset(dataset_reg, select = -c(hardship_reason, inq_last_6mths, num_actv_rev_tl, percent_bc_gt_75, tot_hi_cred_lim))
                  
lmp.fit=lm(int_rate~.,data = dataset_reg)
summary(lmp.fit)

dataset_reg <- subset(dataset_reg, select = -c(bc_util))
                  
```

\pagebreak 
## Main task:
### Compare three different methods to perform regression, using the cross-validation method to compute the best parameters. Consider using some regularization for the parameters shrinkage. Test the train error rate, the CV error rate and the test error.

Stepwise Feature selection

```{r Generalized Linear Model with Stepwise Feature Selection, cache=TRUE, eval=FALSE}
## Read Model from file or compute it 
if (file.exists("fit.glmStepAIC.reg.rds")) {
   fit.bridge.reg <- readRDS("fit.glmStepAIC.reg.rds")
} else {
    fit.glmStepAIC.reg <- train(int_rate~.,
                    data = dataset_reg,
                    method = "glmStepAIC",
                    na.action = na.pass)
}
fit.glmStepAIC.reg
fit.glmStepAIC.reg$finalModel
plot(fit.glmStepAIC.reg$finalModel)

```


Limit the dataset by stepwise feature selection

```{r}

dataset_reg <- subset(dataset_reg, select = c(int_rate, bc_util, delinq_2yrs, installment, initial_list_status, loan_status, revol_util, sub_grade))

```



### Apply the “validation set approach” to reserve a meaningful amount of data for the test phase.

```{r validation set approach regression, cache=TRUE}
# setting a seed for reproducability
set.seed(7)
# We randomly split our dataframe in a train and test set
trainRows = sample(1:nrow(dataset_reg),0.8*nrow(dataset_reg))
testRows = nrow(dataset_reg) - trainRows
train.data.reg = dataset_reg[trainRows,]
test.data.reg = dataset_reg[-trainRows,]


```


```{r cross validation, results=FALSE, cache=TRUE}
control.reg <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
```

```{r Linear Model analysis regression, cache=TRUE}
if (file.exists("fit.lm.reg.rds")) {
   fit.lm.reg <- readRDS("fit.lm.reg.rds")
} else {
  fit.lm.reg <- train(int_rate~.,
                      data = train.data.reg,
                      trControl = control.reg,
                      method = "lm",
                      na.action = na.pass,
                      preProc = c("zv","center","scale"))  
}
fit.lm.reg
fit.lm.reg$finalModel
plot(fit.lm.reg$finalModel)

```

Fit the a extreme gradient boosting model.

```{r}

if (file.exists("fit.egb.reg.rds")) {
   fit.egb.reg <- readRDS("fit.egb.reg.rds")
} else {
    fit.egb.reg = train(int_rate~.,
                    data = train.data.reg,
                    trControl = control.reg,
                    method = "xgbTree"
    )
}

```

Fit the random forest model.

```{r Random Forest}
## Read Model from file or compute it 
if (file.exists("fit.RF.reg.rds")) {
   fit.RF.reg <- readRDS("fit.RF.reg.rds")
} else {
   fit.RF.reg <- train(int_rate~.,
                    data = train.data.reg,
                    trControl = control.reg,
                    method = "parRF",
                    na.action = na.pass)
}

fit.RF.reg
fit.RF.reg$finalModel
plot(fit.RF.reg)
```

```{r Calculating TEST MAPE out of the models}
# MAPE LM TEST
mape_lm_test <- MAPE(predict(fit.lm.reg, test.data.reg), test.data.reg$int_rate)

# MAPE ExtremeGradiantBoosting TEST
mape_egb_test <- MAPE(predict(fit.egb.reg, test.data.reg), test.data.reg$int_rate)

# MAPE Random Forest TEST
mape_rf_test <- MAPE(predict(fit.RF.reg, test.data.reg), test.data.reg$int_rate)

test_mapes <- data.frame("LM" = mape_lm_test, "egb"= mape_egb_test, "Random Forest" = mape_rf_test)

```

```{r Calculating TRAIN MAPE out of the models}
# MAPE LM train
mape_lm_train <- MAPE(predict(fit.lm.reg, train.data.reg), train.data.reg$int_rate)

# MAPE ExtremGradiantBoosting train
mape_egb_train <- MAPE(predict(fit.egb.reg, train.data.reg), train.data.reg$int_rate)

# MAPE Random Forest train
mape_rf_train <- MAPE(predict(fit.RF.reg, train.data.reg), train.data.reg$int_rate)

train_mapes <- data.frame("LM" = mape_lm_train, "egb"= mape_egb_train, "Random Forest" = mape_rf_train)
```

```{r compare MAPE out of the model}
# Grouping Test and Train MAPEs
mapes <- union(test_mapes, train_mapes)
row.names(mapes) <- c("test", "train")
print(mapes)
#As we can see from the table, Random Forest provides us with the lowest value of MAPE for both train and test sets, so we assume that Random Forest is the most accurate model from those being used here to predict interest rate.
```

\pagebreak 
   
# Part 2 - Classification Analysis

Our goal in the second part of the assignment is to predict if a new customer will be able to fully pay back their loans using a classification method. Thus, we concentrate on the "concluded lends" in the data set, i.e., on all lends whose loan_status is not Current. 

## Preparatory tasks

We filter out all observations with loan_status == Current.
For the remaining observations, we check if the loan_status is “Fully Paid”. If not, change the value of loan_status to “DEFAULTED”.

```{r}
# set dataset as data.table::datatable
setDT(dataset)

# filter out all observations with loan_status == Current and storing it in a separate Data
dataset_cla <- dataset[loan_status != 'Current']

# change all the loan status that are not "Fully Paid" to 1
dataset_cla$defaulted[dataset_cla$loan_status != "Fully Paid"] <- 1

# change level of defaulted to 1 and 0 
levels(dataset_cla$defaulted) = c(1, 0)

# Change all the defaulted values that aren't "Default" to 0
dataset_cla$defaulted[is.na(dataset_cla$defaulted)] <- 0

# remove origin variable, because defaulted is relevant now
dataset_cla$loan_status <- NULL

# set defaulted as factor
dataset_cla$defaulted <- as.factor(dataset_cla$defaulted)

# confirm steps below, by checking results
table(dataset_cla$defaulted)
```

### Create a validation set. 

```{r validation set approach classification}
# setting a seed for reproducability
set.seed(7)

# random split into  train and test set, with a ratio of 20:80
trainIndex <- sample(1:nrow(dataset_cla),0.8*nrow(dataset_cla))

train.data.cla <- dataset_cla[trainIndex,]
test.data.cla  <- dataset_cla[-trainIndex,]

```
## Data Exploration

```{r Data Exploration Classification defaulted int_rate}
{ggplot(data = dataset_cla, mapping = aes(defaulted, int_rate)) +
    geom_boxplot()}
# We can see that credits who defaulted generally have a higher int_rate
```

## Main tasks:

Now we can go over to do the analysis on the dataset. Therefore we use different approaches for feature selection (PLS and PCA). Based on the results, we choose the features and do the classification analysis.   

### Use Principal Component Analysis for base transformation and then compare it with the Partial Least Squares Regression result. Select the best base with cross validation, using the better of the two approaches.

```{r cross-validation Classification}
# Compile cross-validation settings
set.seed(100)
myfolds.cla <- createMultiFolds(train.data.cla, k = 5, times = 10)
control.cla <- trainControl("repeatedcv", index = myfolds.cla, selectionFunction = "oneSE")
 
```

Perform Partial Least Squares Regression with caret package, to have a standartized handling.

```{r PLS analysis}

# Train PLS model
fit.pls.cla <- train(defaulted ~ ., data = train.data.cla,
 method = "pls",
 metric = "Accuracy",
 tuneLength = 20,
 trControl = control.cla,
 preProc = c("zv","center","scale"))

plot(fit.pls.cla)

```

Perform Principal Component Analysis with caret package, to have a standartized handling.

```{r PCA analysis}
# PCA-DA
fit.lda.cla <- train(defaulted ~ ., data = train.data.cla,
method = "lda",
metric = "Accuracy",
trControl = control.cla,
preProc = c("zv","center","scale","pca"))
```

Compile models and compare performance

```{r, message=FALSE, warning=FALSE, results="hide"}

models <- resamples(list("PLS-DA" = fit.pls.cla, "PCA-DA" = fit.lda.cla))
bwplot(models, metric = "Accuracy")

plot(varImp(fit.pls.cla), fit.pls.cla$bestTune$ncomp, main = "PLS-DA")

```



```{r, eval=FALSE, results="hide"}
# limiting the variables of the dataset based on the results from PLS analysis in the step before

train.data.cla <- subset(train.data.cla, select = c(defaulted, total_rec_prncp, recoveries, collection_recovery_fee, last_pymnt_amnt, total_pymnt_inv, total_pymnt, debt_settlement_flag, sub_grade, int_rate, out_prncp, out_prncp_inv, settlement_status, term))
test.data.cla  <- subset(test.data.cla, select = c(defaulted, total_rec_prncp, recoveries, collection_recovery_fee, last_pymnt_amnt, total_pymnt_inv, total_pymnt, debt_settlement_flag, sub_grade, int_rate, out_prncp, out_prncp_inv, settlement_status, term))
```

### Perform the classification using KNN, Logistic Regression, Decision Tree and Random Forest.

Train a model with KNN

```{r Perform KNN with caret}
## knn
fit.knn.cla  <- train(defaulted ~ ., 
                data = train.data.cla, 
                method = "knn",
                trControl = control.cla,
                preProc = c("zv","center","scale")
                ) 

```

Train a model with Logistic Regression

```{r Perform Logistic Regression with caret, message=FALSE, warning=FALSE}
## logistic regression
fit.lreg.cla <-train(defaulted ~ recoveries + collection_recovery_fee + total_rec_prncp 
+ last_pymnt_amnt + total_pymnt, 
                data = train.data.cla, 
                method = "glm",
                trControl = control.cla,
                family=binomial(),
                preProc = c("zv","center","scale")
                ) 

```

Train a model with Decision Trees

```{r Perform Decision tree with caret}
## decision tree 
fit.dtree.cla <-train(defaulted ~ recoveries + collection_recovery_fee + total_rec_prncp 
+ last_pymnt_amnt + total_pymnt, 
               data=train.data.cla,
               method="ctree",
               trControl = control.cla,
               preProc = c("zv","center","scale"))
```

Train a model with Random Forest

```{r Perform Random Forest with caret}
## random forest 
fit.rf.cla <-train(defaulted ~ recoveries + collection_recovery_fee + total_rec_prncp 
+ last_pymnt_amnt + total_pymnt, 
               data=train.data.cla,
               method="rf",
               trControl = control.cla,
               preProc = c("zv","center","scale"))
```

### Compare the respective train and test error performances to select one of these approaches.

```{r ROC Plot}
par(pty = "s")
roc(test.data.cla$defaulted, as.numeric(predict(fit.dtree.cla, 
test.data.cla)), plot=TRUE, legacy.axes=TRUE, 
    percent=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage", col="#377eb8", 
    lwd=4, print.auc = TRUE, print.auc.y=80, print.auc.x=30)

plot.roc(test.data.cla$defaulted, as.numeric(predict(fit.rf.cla, 
test.data.cla)), percent=TRUE, col="#b8377e", 
         lwd=4, print.auc=TRUE, add=TRUE, print.auc.y=70, print.auc.x=30)

plot.roc(test.data.cla$defaulted, as.numeric(predict(fit.lreg.cla, test.data.cla)), 
         percent=TRUE, col="#b87137", lwd=4, print.auc=TRUE, add=TRUE, print.auc.y=60, print.auc.x=30)

plot.roc(test.data.cla$defaulted, as.numeric(predict(fit.knn.cla, test.data.cla)), 
         percent=TRUE, col="#7eb837", lwd=4, print.auc=TRUE, add=TRUE, print.auc.y=50, print.auc.x=30)
	
	legend("bottomright", legend=c("Decission Tree", 
	"Random Forest", "Logisitic Regression", "KNN"), 
	col=c("#377eb8", "#b8377e", "#b87137", "#7eb837"), lwd=4)
	
```

### Perform the prediction on the validation set and compute the confusion matrix.

```{r Perform Confusion Matrix for each method}

#Confusion Matrix KNN
confusionMatrix( predict(fit.knn.cla, test.data.cla), test.data.cla$defaulted)    #Test

#Confusion Matrix Logistic Regression
confusionMatrix( predict(fit.lreg.cla, test.data.cla), test.data.cla$defaulted)   #Test

#Confusion Matrix Decision Trees
confusionMatrix( predict(fit.dtree.cla, test.data.cla), test.data.cla$defaulted)  #Test

#Confusion Matrix Random Forest
confusionMatrix( predict(fit.rf.cla, test.data.cla), test.data.cla$defaulted)     #Test
```

\pagebreak 

# Conceptually comparison of our  approach with a solution existing for this problem

We will compare our approach to the following one:
Forecasting Credit Default Probability
Author: Matthew Ludwig
Date: 11 May 2017
URL: <https://rstudio-pubs-static.s3.amazonaws.com/275340_24f229732ac04bf182ccae5fffdfb47a.html>


Replacing missing data:
In our case missing data for int_rate was simply removed, other variables that did have missing data we treated as follows:
- for numericals values we dummy coded them with a '-1' in the approach.
- for factors we replaced missing values with 'unknown'

In the approach we're comparing the int_rate used a similar approach named "Missing Not At Random".
Which basically replaced the missing int_rate value by 'Missing'

Outliers:
We kept all the outliers and did not try to get these out of our data.
In the approach we are comparing, outliers were removed. 
 
Sampling:

We first started with a very small subset (1000 rows) to get ahold if the models we were running were doing so correctly. In our final approach we are using 20000 rows as using any more is too computationaly intensive for the hardware we have.

In the approach we are comparing to the whole dataset is of 29092 row. 20000 are used for training and 9092 are used from testing.

Parameter selection 
To find out which parameters were useful, we used a logistic regression and a stepwise regression.

This helped us to select a handful set of predictors that were deemed relevant for our models based upon the p-value.
We can see that this is the same approach that was taken by Matthew Ludwig.

Testing:

We used a crossvalidation set approach, and splitted the data in a ratio of 80/20(train/test). 

 
 Model:
 
 We used various models to find which fit better for clasifying, namely:
 - knn
 - random forest
 - logistic regression
 - descision tree
 
 The approach we are comparing to used one model which is as follows:
 
 "Final_Model <- glm(loan_status ~ loan_amnt + grade + annual_inc + int_bin + emp_bin, family = "binomial", data = training)"
# Summary 

We first generated a subset of the original lending-club-loan data and saved that file. 

With the new subset we started with exploratory data analysis, based on correlation matrix and, boxplots and plots we could identify important variables and exclude correlating ones. Based on the forgoing data exploration and checking the DataDescription we limited the amount of feautres. For case of simplicity and available computation power we further had to limit the features once again. 

The toplevel approach for the default prediction refers to the CRIPS for Data Mining  <https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining> and is similar to most of the approaches found on the internet. After a general Data Preparation that is identical for the regression and classifiction task, we did the specific classification data preparation, as changing dependent variable to a 2-factor (1,0) variable. Then the validation set approach was used to split the data into train & test data, to generally improve the model performance. With the Principal Component Analysis &  Partial Least Squares we limited the amount of features to consider. Furthermore four different models were used (KNN, Logistic Regression, Decision Trees, Random Forest) and the performance compared with a ROC curve. 

Nevertheless, the analysis shows that it's helpful with machine learning techniques to predict the interest rate and probability that a loan is defaulted. For us the Random Forest works the best for interest rate prediction while logistic regression has the best results for predicting default status. At the ROC curve, we get the best true positive vs false postive ratio. 

## Room for improvement

In future work, natural language processing (NLP) could be used to extract informatcion from Loan description or the job title. 
Feature engineering could be implemented, e. g. the zip_code could be used to determine the unemployment rate. 
Furthermore it would be interesting to spend more time on the performance tuning of each model, by using different parameters for the fitting. 
