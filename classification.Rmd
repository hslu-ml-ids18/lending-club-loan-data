---
title: "Group 7 - Classification model for the default status"
author: "L. Becker, A. Chebatarova, A. Kandel, A.Kusche, R. Mizrak"
date: 'Sys.Date()'
output:
  pdf_document: default
  html_document:
    df_print: paged
---

## Initialization

```{r initialization, results="hide"}
source(file = "functions.R")
func_init_env()
```

```{r loading Rds file, results="hide"}
dataset <- func_data_load()
```

```{r preprocessing, echo=TRUE}
dataset <- func_data_prep(dataset)
```

## Preprocessing

```{r concluded loans results= FALSE}
# Make a data.table
setDT(dataset)

# filter out all observations with loan_status == Current
dataset <- dataset[loan_status != 'Current']

# Change all the loan status that are not "Fully Paid" to 1 = "Default"
dataset$defaulted[dataset$loan_status != "Fully Paid"] <- 1

# Chaning levels of defaulted to have "Default" and "Paying"
levels(dataset$defaulted) = c(1, 0)

# Change all the defaulted values that aren't "Default" to 
dataset$defaulted[is.na(dataset$defaulted)] <- 0

# Remove unused factor levels
dataset$defaulted <- factor(dataset$defaulted)


# # Removing Feature 
# dataset <- subset(dataset, select = -c(hardship_type, hardship_status, hardship_loan_status,settlement_status, verification_status_joint, hardship_flag, disbursement_method, debt_settlement_flag, application_type, purpose, pymnt_plan, verification_status, home_ownership, initial_list_status, term))
```

```{r}

# Dummy coding 
dataset <- dummy.data.frame(dataset, names = c("application_type","debt_settlement_flag", "disbursement_method","hardship_flag","hardship_loan_status","hardship_type","home_ownership","initial_list_status","loan_status","purpose","term", "settlement_status"))

dataset <- dummy.data.frame(dataset, names = c("hardship_status","pymnt_plan", "settlement_status", "verification_status", "verification_status_joint"))
```

## Create Train and Testset

```{r validation set approach}
# setting a seed for reproducability
set.seed(7)

# random split into  train and test set, with a ratio of 20:80
trainIndex <- sample(1:nrow(dataset),0.8*nrow(dataset))

train.data <- dataset[trainIndex,]
test.data  <- dataset[-trainIndex,]

# creating a small subset of data for testing models
train_smallIndex  <- sample(1:nrow(train.data), 1000)
test_smallIndex   <- sample(1:nrow(test.data), 1000)

small_train <- dataset[train_smallIndex,]
small_test  <- dataset[test_smallIndex,]
```

# https://www.r-bloggers.com/partial-least-squares-in-r/
```{r}
# Compile cross-validation settings
set.seed(100)
myfolds <- createMultiFolds(train.data, k = 5, times = 10)
control <- trainControl("repeatedcv", index = myfolds, selectionFunction = "oneSE")
 
```

#Perform Partial Least Squares Regression
```{r}

# Train PLS model
mod1 <- train(defaulted ~ ., data = train.data,
 method = "pls",
 metric = "Accuracy",
 tuneLength = 20,
 trControl = control,
 preProc = c("zv","center","scale"))

```

#Perform Principal Component Analysis 
```{r}
# PCA-DA
mod2 <- train(defaulted ~ ., data = train.data,
method = "lda",
metric = "Accuracy",
trControl = control,
preProc = c("zv","center","scale","pca"))
 
plot(mod1)

```

# Compile models and compare performance
```{r, message=FALSE, warning=FALSE, results="hide"}

models <- resamples(list("PLS-DA" = mod1, "PCA-DA" = mod2))
bwplot(models, metric = "Accuracy")

plot(varImp(mod1), 10, main = "PLS-DA")
plot(varImp(mod2), 10, main = "PCA-DA")

```

```{r}
lm.fit <- lm(defaulted ~., data = dataset, type = binomial)

lm.fit <- lm(defaulted ~., data = subset(dataset, select = -c(loan_status)))
summary(lm.fit)
```
Perform the classification using Decision tree.
```{r Fit a regression tree to predict the defaulted status}
tree.defaulted<- tree(defaulted ~., data = subset(dataset, select = -c(id_2)))
summary(tree.defaulted)
plot(tree.defaulted)
text(tree.defaulted, cex = 0.75)
# The text isn't showing up
```
Perform the classification using Logistic Regression.
```{r classification using Logistic Regression, results= FALSE}
# Setting Seed
set.seed(7)
# Creating a Generalized Linear Model
fit_glm_defaulted <- glm(defaulted ~., family = binomial(link = "logit"), data= small_train, na = na.omit)

# Summary of theeneralized Linear Model
summary(fit_glm_defaulted)

# Anova
# anova(fit_glm_defaulted, test="Chisq")
# The model could not converge, meaning that we should improve our variable selection.

# creating predictions for the Generalized Linear Model
glm.probs <- predict(fit_glm_defaulted, type = "response")
glm.probs[1:5]

# setting if the prob is higher than 0.5 = 1 (defaulted), if bellow = 0 (Paying)
glm.pred <- ifelse(glm.probs > 0.5, "1", "0")

# Calculating the mean of preds == response
mean(glm.pred == defaulted_int_rate)

# We get an acc rate of 0.6442497/ error rate of 35.6%

```
Perform the classification using Logistic Regression.
```{r classification using Logistic Regression with manual factor selection}
# Setting Seed
set.seed(7)
# Creating a Generalized Linear Model
fit_glm_defaulted <- glm(defaulted ~ sub_grade + collection_recovery_fee + hardship_loan_status , family = binomial(link = "logit"), data= small_train, na = na.omit)

# Summary of theeneralized Linear Model
summary(fit_glm_defaulted)

# Anova
anova(fit_glm_defaulted, test="Chisq")
# The model could not converge, meaning that we should improve our variable selection.

# creating predictions for the Generalized Linear Model
glm.probs <- predict(fit_glm_defaulted, type = "response")
glm.probs[1:5]

# setting if the prob is higher than 0.5 = 1 (defaulted), if bellow = 0 (Paying)
glm.pred <- ifelse(glm.probs > 0.5, "1", "0")

# Calculating the mean of preds == response
mean(glm.pred == defaulted_int_rate)

# We get an acc rate of 0.6952841 which is better than the larger model.

```
Perform the classification using KNN.
```{r k-NN, cache = TRUE}
# Set random seed.
set.seed(7)

# Creating test and train labels
train_labels <- small_train$defaulted
test_labels <- small_test$defaulted

# 
knn_train <- small_train
knn_test <- small_test

# droping defaulted columns
knn_train$defaulted <- NULL
knn_test$defaulted <- NULL

# Make predictions using knn: pred
knn_pred <- knn(train = knn_train, test = knn_test, cl = train_labels, k = 1)

# Construct the confusion matrix: conf
conf <- table(test_labels, pred)

# Print out the confusion matrix
conf
# Something isn't right in the confusion matrix.
# Plot the ROC curves #something s
pred <- prediction(preds_list, train_labels)
rocs <- performance(pred, "fpr", "tpr")
plot(rocs, col = as.list(1:m), main = "Test Set ROC Curves")

# Define range and accs
# range <- 1:round(0.2 * nrow(knn_train))
# accs <- rep(0, length(range))
# for (k in range) {
# 
#   # Make predictions using knn: pred
#   knn_pred <- knn(knn_train, knn_test, train_labels, k = k)
# 
#   # Construct the confusion matrix: conf
#   conf <- table(test_labels, knn_pred)
# 
#   # Calculate the accuracy and store it in accs[k]
#   accs[k] <- sum(diag(conf)) / sum(conf)
# }

saveRDS(accs, "knn_accs")
saveRDS(knn_pred, "knn_pred")
saveRDS(conf, "knn_conf")

# Calculate the best k
which.max(accs1)

# Plot the accuracies.
# Highlighting the datapoint that has the higgest accuracy 
plot(range1, accs1, xlab = "k", ylab = "Accuracy", main = "Number of neighbours (K) vs Test Accuracy",
     col=ifelse(range1== which.max(accs1), "red", "black"))

# with the newer model we get k = 5, probably beaucse with this bigger dataset we don't need as many neighbourgs

# Estimate the test error rate
mean(knn_pred != test_labels)
# we get 0.2222156

# Fit the k-NN model with k=5
set.seed(7)
knn_5_pred <- knn(train = knn_train, test = knn_test, cl = train_labels, k = 5)
# Construct the confusion matrix: conf
conf <- table(test_labels, knn_pred)
print(conf)
mean(knn_pred != test_labels)

```
Perform the classification using Random forest.
```{r }
# Set random seed.
set.seed(7)

defaulted_RF <-  foreach(ntree=rep(125, 4), .combine=combine, .packages= c('magrittr', 'randomForest')) %dopar% {
defaulted_RF <- randomForest(defaulted ~. , data=dataset , subset =  small_trainRows, importance=TRUE, do.trace = TRUE, ntree=ntree)
 }
defaulted_RF
summary(defaulted_RF)
plot(defaulted_RF)
fitted.int_rate=predict(defaulted_RF)

defaulted_RF_pred <- predict(defaulted_RF, newdata = dataset[small_testRows,])
view(defaulted_RF_pred)

defaulted_int_rate <- small_train$defaulted
ROC <- roc(defaulted_RF_pred, defaulted_int_rate)
plot(ROC, col = "red")
auc(ROC)
```
Compare the respective train and test error performances to select one of these approaches.
```{r Compare the respective train and test error performances to select one of these approaches}


# List of predictions
glm.pred.roc <- as.integer(glm.pred)
glm.pred.roc1 <- head(glm.pred, -1)
glm.pred.roc1 <- as.integer(glm.pred.roc1)

# Getting the mean  error rate for all models

# KNN 1
mean(knn_pred != test_labels)
# We have an error rate of 20.27 % with Knn = 1

# KNN 5
mean(knn_5_pred != test_labels)
# We have an error rate of 19.38 % with Knn = 5

# Logistic Regression
mean(glm.pred.roc1 != test_labels)
# We have an rror rate of 35.6% for with Logistic Regression


# missmatch in number of items, removing last one of glm.pred to match the numbers with knn_pred
knn_pred_roc <- as.double(knn_pred)

preds_list <- list(knn_pred, knn_5_pred, glm.pred.roc1) # add defaulted_RF_pred

# List of actual values (same for all)
m <- length(preds_list)
actuals_list <- rep(list(test_labels), m)

# Plot the ROC curves #something is wrong
pred <- prediction(preds_list, actuals_list)
rocs <- performance(pred, "tpr", "fpr")
{plot(rocs, col = as.list(1:m), main = "Test Set ROC Curves")
legend(x = "bottomright", 
       legend = c("knn_1_pred", "knn_5_pred", "GLM"),
       fill = 1:m)}
```

Perform the prediction on the validation set and compute the confusion matrix.
```{r }
# KNN

```

Conceptually compare your approach with a solution existing for this problem. (Default
prediction is a very well-known problem in literature).
```{r }

```
